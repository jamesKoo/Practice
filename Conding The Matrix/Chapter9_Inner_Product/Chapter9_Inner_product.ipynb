{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter9 - Inner Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "길이(length)와 직교(perpendicular)의 개념이 수학적 용어로 어떻게 해석되는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 소방차 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "집 좌표 [2,4]와 도로는 원점과 점 [6,2]을 지나는 직선을 따라 있다. 집에서 가장 가까운 직선상의 어떤 지점으로 소방차를 몰고 갈 수 있으면, 불에 타고 있는 집을 구할 만큼 거리가 충분히 작을 것인가?  \n",
    "이 것을 계산문제로 구성해 보자. 도로는 직선 $\\{\\alpha[3,1]:\\alpha \\in \\mathbb R\\}$을 따라 지나간다. 이 소방차 문제는 다음과 같이 구성될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Problem 9.1.1\n",
    "하나의 주어진 벡터에 가장 가까운 다른 하나의 주어진 벡터의 생성 내에 있는 벡터(소방차 문제라고도 알려짐).\n",
    "- input: 벡터 v 및 b\n",
    "- output: b에 가장 가까운 직선 $\\{\\alpha v:\\alpha \\in \\mathbb R\\}$ 상의 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1.1 거리, 길이, norm, 내적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 벡터 p와 b 사이의 **거리**는 차분 p-b의 **길이**로 정의할 것이다.  \n",
    "벡터에 대해 **길이**는 **norm**을 사용한다.  \n",
    "벡터 v의 **norm**은 $\\left\\|v\\right\\|$로 표현된다. norm은 길이의 역할을 하므로, 다음의 norm 성질을 만족해야 한다.\n",
    "\n",
    "- Property N1: 임의의 벡터 v에 대해, $\\left\\|v\\right\\|$은 영이 아닌 실수이다.\n",
    "- Property N2: 임의의 벡터 v에 대해, $\\left\\|v\\right\\|$이 영이 될 필요충분조건은 v가 영벡터인 것이다.\n",
    "- Property N3: 임의의 벡터 v에 대해, 스칼라 $\\alpha$에 대해, $\\left\\|\\alpha v\\right\\|=\\left|\\alpha\\right|\\left\\|v\\right\\|$ 이다.\n",
    "- Property N4: 임의의 벡터 u, v에 대해, $\\left\\|u+v\\right\\|\\leq \\left\\|u\\right\\|+\\left\\|v\\right\\|$\n",
    "\n",
    "벡터의 norm을 정의하는 한 가지 방법은 내적(inner product)이라고 하는 벡터들에 대한 연산을 정의하는 것이다. 벡터 u와 v의 내적에 대한 표기접은 아래와 같다.  \n",
    "$\\quad <u,v>$\n",
    "\n",
    "내적이 정의되면, 벡터 v의 norm은 다음과 같이 정의된다.  \n",
    "$\\quad \\left\\|v\\right\\|=\\sqrt{<v,v>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 실수 벡터들에 대한 내적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb R$상의 벡터들에 대한 내적은 도트곱으로 정의된다.  \n",
    "$\\quad <u,v>=u\\cdot v$\n",
    "\n",
    "실수 벡터들에 대한 내적ㅇ의 몇몇 대수적 성질들은 도트곱의 성질들을 쉽게 따른다.\n",
    "- 첫 번째 인자의 선형성(linearity in the first argument): $<u+v,w>=<u,w>+<v,w>$\n",
    "- 대칭성(symmetry): $<u,v>=<v,u>$\n",
    "- 동질성(homogeneity): $<\\alpha u,v>=\\alpha<u,v>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2.1 실수 벡터들의 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실수 벡터들의 norm 함수는 도트곱으로 정의되어 v가 D-벡터이면, 아래와 같이 표현된다.  \n",
    "$\\quad \\left\\|v\\right\\|=\\sqrt{\\sum_{i\\in D}{v_{i}^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.2.1\n",
    "2-벡터의 예를 고려해 보자. 벡터 $u=[u_1,u_2]$의 길이는 무엇인가? 피타고라스 정리를 기억해 보자. 직각 삼각형의 세 변의 길이가 a,b,c이고 c가 빗변의 길이이면 다음이 성립한다.  \n",
    "$\\quad a^2+b^2=c^2$  \n",
    "이 식을 사용하여 u의 길이를 계산하 수 있다.  \n",
    "$\\quad \\left\\|v\\right\\|^2 = u_1^2+u_2^2$  \n",
    "따라서, 길이에 대한 이러한 개념은 적어도 $\\mathbb R^2$의 벡터들에 대한 길이와 일치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.3 직교성(Orthogonality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직교(Otrhogonal)는 수직(perpendicular)에 대한 수학적인 용어이다.  \n",
    "\n",
    "\n",
    "임의의 두 벡터 u, v가 있고, u+v의 제곱의 길이는 다음과 같다.  \n",
    "$\\quad \\left\\|u+v\\right\\|^2= <u+v,u+v>$  \n",
    "$\\quad \\quad \\quad \\quad = <u,u+v>+<v,u+v>$  \n",
    "$\\quad \\quad \\quad \\quad = <u,u>+<u,v>+<v,u>+<v,v>$  \n",
    "$\\quad \\quad \\quad \\quad = \\left\\|u\\right\\|^2+2<u,v>+\\left\\|v\\right\\|^2$\n",
    "\n",
    "피타고라스 정리를 적용하면 마지막 표현이 $\\left\\|u\\right\\|^2+\\left\\|v\\right\\|^2$이 될 필요충분조건은 $<u,v>=0$이 되는 것이다.\n",
    "\n",
    "**만약 $<u,v>=0$이면 u와 v는 직교라고 정의한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 9.3.1 실수 벡터들에 대한 피타고라스 정리\n",
    "만약 실수 벡터 u와 v가 직교하면 다음이 성립한다.  \n",
    "$\\quad \\left\\|u+v\\right\\|^2=\\left\\|u\\right\\|^2+\\left\\|v\\right\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.1 직교성의 성질"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma 9.3.2 Orthogonality Properties\n",
    "임의의 백터 u와 v, 그리고 임의의 스칼라 $\\alpha$에 대해,\n",
    "- Property O1: 만약 u가 v와 직교하면 $\\alpha u$는 모든 스칼라 $\\alpha$에 대해 $\\alpha v$와 직교한다.\n",
    "- Property O2: 만약 u와 v 둘다 w와 직교하면 u+v는 w와 직교한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma 9.3.3\n",
    "만약 u가 v와 직교이면 임의의 스칼라 $\\alpha, \\beta$에 대해 다음이 성립한다.  \n",
    "$\\quad \\left\\|\\alpha u+\\beta v\\right\\|^2=\\alpha^2\\left\\|u\\right\\|^2+\\beta^2\\left\\|v\\right\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.3.4\n",
    "만약 u와 v가 직교라는 조건을 제거하면 Lemma 9.3.3은 참이 아니라는 것을 수치적 예를 사용하여 보여라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u=[1,2], v=[2,1], a=2, b=2\n",
    "# ||au+bv||^2 = ||[6,6]||^2 = 72\n",
    "# Lemma 9.3.3 -> ||u+v||^2 = 4||[1,2]||^2 + 4||[2,1]||^2 = 40\n",
    "# -> 72 != 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.3.5\n",
    "귀납법과 Lemma 9.3.3을 사용하여 다음의 일반화를 증명해 보자. $v_1,...,v_n$은 서로 직교한다고 해 보자. 임의의 계수 $\\alpha_1,...,\\alpha_n$에 대해 다음이 성립한다.\n",
    "$\\quad \\left\\|\\alpha_1v_1+...+\\alpha_n v_n\\right\\|^2=\\alpha_1^2\\left\\|u_1\\right\\|^2+...+\\alpha_n^2\\left\\|v_n\\right\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "n이 1일때,  \n",
    "$\\quad \\left\\|\\alpha_1v_1\\right\\|^2=\\alpha_1^2\\left\\|u_1\\right\\|^2$  동질성에 의해  \n",
    "\n",
    "\n",
    "n이 k일때 성립한다면 k+1일때,  \n",
    "$\\quad \\left\\|\\alpha_1v_1+...+\\alpha_k v_k\\right\\|^2=\\alpha_1^2\\left\\|v_1\\right\\|^2+...+\\alpha_k^2\\left\\|v_k\\right\\|^2$  \n",
    "\n",
    "$\\alpha_1v_1+...+\\alpha_k v_k$를 u벡터라 하면  \n",
    "$\\quad \\left\\|u+\\alpha_{k+1}v_{k+1}\\right\\|^2=\\left\\|u\\right\\|^2+\\alpha_{k+1}^2\\left\\|v_{k+1}\\right\\|^2$  \n",
    "$\\quad \\quad\\quad\\quad\\quad\\quad\\quad=\\alpha_1^2\\left\\|v_1\\right\\|^2+...+\\alpha_k^2\\left\\|v_k\\right\\|^2+\\alpha_{k+1}^2\\left\\|v_{k+1}\\right\\|^2$  따라서 모든 실수에 대해 성립한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.2 평행 및 수직 성분으로 벡터 분해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition 9.3.6\n",
    "임의의 벡터 b와 v에 대해, 만약 다응ㅁ이 성립하면 벡터 $b^{\\parallel v}$와 $b^{\\bot v}$은 각각 b의 v를 따른 투영(projection along v)과 b의 v에 직교하는 투영(projection orthogonal to v)이라 정의한다.\n",
    "\n",
    "$\\quad b=b^{\\parallel v} + b^{\\bot v}$\n",
    "\n",
    "여기서, 어떤 스칼라 $\\sigma \\in \\mathbb R$에 대해,  \n",
    "$\\quad b^{\\parallel v}=\\sigma v$\n",
    "\n",
    "이고  \n",
    "$\\quad b^{\\bot v}$은 v에 직교한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.3.7\n",
    "평면을 고려하고, 직선은 x-축, 즉 집합 $\\{(x,y): y=0\\}$이라고 해 보자. b는 $(b_1,b_2)$이고 v는 (1,0)이라 해보자. v를 따른 b의 투영은 $(b_1,0)$이고, v에 직교하는 b의 투영은 $(0,b_2)$이다.\n",
    "\n",
    "정의 9.3.6이 맞는지 고려해 보자.\n",
    "- $(b_1,b_2)=(b_1,0)+(0,b_2)$ 이것은 당연히 참이다.\n",
    "- $(b_1,0)=\\sigma(1,0)$이어야 하고, 이것은 $\\sigma$가 $b_1$이 되게 선택될 경우 참이다.\n",
    "- $(0,b_1)$이 (1,0)에 직교해햐 하고, 이것은 명백히 참이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.3 소방차 문제 대한 해의 직교 성질"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma 9.3.8 Fire Engine Lemma\n",
    "b와 v는 벡터라고 하자. b에 가장 가까운 Span{v}내의 점은 $b^{\\parallel v}$이고, 그 거리는 $\\left\\|b^{\\bot v}\\right\\|$ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.3.9\n",
    "이전 예제를 가지고 계속해 보자. Lemma에 따르면 직선 Span{(1,0)}상에 있으며 $(b_1,b_2)$에 가장 가까운 점은 $b^{\\parallel v}=(b_1,0)$이다.\n",
    "\n",
    "임의의 다른 점 p에 대해, 점 $(b_1,b_2), b^{\\parallel v}$, 그리고 p는 직각 삼각형을 형성한다. $b^{\\parallel v}$와 p는 밑변을, b와 $b^{\\parallel v}$은 높이를, b와 p는 빗변를 나타낸다. 피타고라스 정리에 의하면 높이는 빗변보다 작다. 이것은 p가 $b^{\\parallel v}$보다 $(b_1,b_2)$에서 더 멀리 떨어져 있다는 것을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.3.10\n",
    "v가 영벡터인 경우는 어떤가? $b^{\\parallel v}$는 영벡터이고, $b^{\\bot v}$은 단순히 b이고 거리는 $\\left\\|b\\right\\|$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.4 투영 및 가장 가까운 점 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma 9.3.11 임의의 실수 벡터 b와 v에 대해,\n",
    "1. $b-\\sigma v$가 v에 직교하는 그러한 $\\sigma$가 있다.\n",
    "2. Span{v}상에 있으며  $\\left\\| b-p\\right\\|$을 최화하는 점 p는 $\\sigma v$이다.\n",
    "3. $\\sigma$의 값은 $\\frac{<b,v>}{<v,v>}$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 9.3.12 \n",
    "project_along(b, v)을 작성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({0, 1},{0: 1.0, 1: 0.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vecutil import list2vec\n",
    "\n",
    "def project_along(b, v): return ((b*v)/(v*v) if v*v > 1e-20 else 0) * v\n",
    "\n",
    "b = list2vec([1,1])\n",
    "v = list2vec([1,0])\n",
    "project_along(b, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 9.3.13\n",
    "project_orthogonal_1(b, v)을 작성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({0, 1},{0: 0.0, 1: 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def project_orthogonal_1(b, v): return b - project_along(b, v)\n",
    "\n",
    "project_orthogonal_1(b,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.5 소방차 문제에 대한 솔루션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.3.14\n",
    "소방차 문제로 돌아가 보자. v=[6,2]이고 b=[2,4]이다. 직선 $\\{\\alpha v: \\alpha \\in \\mathbb R\\}$위에 있는 가장 가까운 점은 $\\sigma v$이며, $\\sigma$는 다음과 같다.  \n",
    "$\\quad \\sigma = \\frac{b\\cdot v}{v\\cdot v} = \\frac{1}{2}$  \n",
    "따라서, b에 가장 가까운 점은 $\\frac{1}{2}[6,2]=[3,1]$이다. b 까지의 거리는  \n",
    "$\\quad \\left\\|[2,4]-[3,1]\\right\\|=\\left\\|[-1,3]\\right\\|=\\sqrt{10}$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "'가장 근접하는'이란 가장 가깝다는 의미이다. 아프로 다룰 장에버 몇 번 나올 것이다.\n",
    "- least-squares / regression\n",
    "- image compression\n",
    "- principal component analysis\n",
    "- latent sementic analysis of information retrieval\n",
    "- compressed sensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.6 외적(Outer product)과 투영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 u와 v의 *외적*은 행렬-행렬 곱 $uv^T$으로 정의된다.  \n",
    "$\\quad \\begin{bmatrix}  \\\\ u \\\\  \\\\ \\end{bmatrix}\\begin{bmatrix} && v^T &&  \\end{bmatrix}$\n",
    "\n",
    "영이 아닌 벡터 v에 대해, 투영 함수 $\\pi_v:\\mathbb R^n \\rightarrow \\mathbb R^n$을 다음과 같이 정의한다.  \n",
    "$\\quad \\pi_v(x)=x의 v에 대한 투영$\n",
    "\n",
    "$\\left\\|v\\right\\|=1$ 이라고 가정해 보자. 그러면, 투영에 대한 식은 더욱 더 간단해 진다.  \n",
    "$\\quad \\pi_v(x)=(v\\cdot x)v$  \n",
    "$\\quad \\quad \\quad=\\left(\\begin{bmatrix}  \\\\ u \\\\  \\\\ \\end{bmatrix}\\begin{bmatrix} && v^T &&  \\end{bmatrix}\\right)\\begin{bmatrix}  \\\\ x \\\\  \\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.3.15\n",
    "projection_matrix(v)을 작성해 보자. 주어진 벡터 v에 대해 $\\pi_v(x)=Mx$를 만족하는 그러한 행렬 M을 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mat(({0, 1}, {0, 1}), {(0, 0): 0.25, (0, 1): 0.25, (1, 0): 0.25, (1, 1): 0.25})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matutil import rowdict2mat, coldict2mat\n",
    "\n",
    "def projection_matrix(v):\n",
    "    v = (1/(v*v))*v\n",
    "    M = coldict2mat({0:v})*rowdict2mat({0:v})\n",
    "    return M \n",
    "\n",
    "v = list2vec([1,1])\n",
    "projection_matrix(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.3.16\n",
    "v는 영이 아닌 n-벡터라고 해 보자. $\\pi_v(x)=Mx$를 만족하는 행렬 M의 랭크는 무엇인가? 행렬-벡터 또는 행렬-행렬 곱셈에 대한 적절한 해석을 사용하여 답변을 설명해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimv = n\n",
    "# M = (nx1) x (1xn) = n x n, rank(nx1) = 1\n",
    "# 행렬-벡터 정의에 의해 벡터의 선형 결합으로 된다. 즉, vv^T는 v벡터의 선형결합으로 모두 이루어진다.\n",
    "# 따라서 rank(M) = 1이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.3.17\n",
    "v는 영이 아닌 n-벡터라고 해 보자. M은 $\\pi_v(x)=Mx$를 만족하는 행렬이라 하자.\n",
    "1. M과 v를 곱하기 위해 스칼라-스칼라 곱셈이 몇 개나 필요한다.\n",
    "2. x는 열벡터, 즉 n x 1 행렬에 의해 표현된다고 해보자. 두 개의 행렬 $M_1,M_2$가 있다. $M_1(M_2x)$를 계산함으로써 $\\pi_v(x)$를 계산하는 데는 오직 2n개의 스칼라-스칼라 곱셈이 필요하다. 이것을 설명해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "# M = n x n\n",
    "# Mx는 한 행당 n번의 곱셈이 필요하고 총 행은 n개 이므로 n^2 만큼 스칼라-스칼라 곱셈이 필요하다.\n",
    "\n",
    "# 2.\n",
    "# M = (nx1) x (1xn) = M1 x M2\n",
    "# M1(M2x) -> (nx1) x (1xn)x로 \n",
    "# M2x는 n번 곱셈으로 스칼라 값이 나오고 따라서 M1*scalar는 n번 곱셈으로 n-벡터가 나온다.\n",
    "# 따라서 2n번의 곱셈이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3.7 차원이 더 높은 경우에 대한 해결책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반화는 주어진 몇몇 벡터들의 생성에 있으며 주어진 벡터 b에 가장 가까운 벡터를 찾는 것이다. 이러한 계산문제에 대한 하나의 접근방식을 살펴볼 것이며, 그것은 그래디언트 디센트(gradient descent)에 기반을 두고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4 Lab: 기계학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래디언트 디센트법은 비선형함수를 거의 최소화하는 점을 찾는 데 유용하다. 이 방법은 매 이터레이션마다 비선형함수를 선형함수에 의해 근사시킨다.  \n",
    "이러한 특정 함수에 대해 최상의 점을 찾는 훨씬 더 빠르고 직접적인 방법이 있다. 직교화(orthogonalization)에서 이것을 다룰 것이다. 하지만, 그래디언트 디센트법은 좀 더 일반적인 관점에서 유용하고 알아둘 가치가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.1 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위스콘신 유방암 진단 데이터 일부가 주어진다. 각 표본은 30개의 엔트리를 가진 벡터로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.1 \n",
    "read_training_data를 사용하여 train.data 파일에 있는 데이터를 변수 A,b로 읽어들여 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of patient: 300\n",
      "\n",
      " patient ids: {862722, 8811523, 868871, 84501001, 843786, 84667401, 855563, 8912909, 891923, 848406, 854039, 8712729, 84799002, 8860702, 8810528, 86561, 8711202, 88199202, 864292, 857637, 863270, 87281702, 89122, 891936, 856106, 869931, 846381, 8610862, 8612399, 8711216, 84862001, 851509, 862261, 88411702, 867387, 859196, 889403, 842302, 859711, 8712766, 883263, 87106, 857155, 857156, 859717, 883270, 844359, 852552, 855625, 87139402, 8813129, 875093, 87127, 875099, 8610908, 855133, 8712289, 855138, 8712291, 8610404, 869476, 88725602, 84300903, 861799, 877159, 87556202, 853612, 864877, 873586, 88518501, 849014, 873592, 873593, 8711803, 87163, 87164, 855167, 868999, 8912521, 883852, 8910988, 88143502, 865423, 881094802, 859283, 8910996, 8712853, 852631, 865432, 8913049, 866458, 8511133, 861853, 88147101, 872608, 88147102, 8910499, 874662, 8910506, 874158, 8810158, 872113, 8912049, 886452, 844981, 88299702, 885429, 8912055, 88995002, 865468, 8510653, 879804, 86208, 857793, 88350402, 86211, 881861, 878796, 86135501, 86135502, 8810703, 853201, 857810, 85715, 871122, 884437, 879830, 884948, 8913, 871641, 84458202, 871642, 88119002, 884448, 8611555, 873701, 874217, 854253, 871149, 8612080, 864496, 869104, 862965, 892214, 88203002, 888570, 854268, 887549, 857343, 875263, 84358402, 8811779, 862980, 86973701, 86730502, 86973702, 88147202, 88249602, 862989, 8812816, 864018, 8812818, 842517, 862485, 88206102, 891670, 852763, 857373, 857374, 892189, 864033, 871201, 866083, 85713702, 8710441, 88649001, 8812844, 852781, 857392, 89143601, 89143602, 881972, 863030, 863031, 882488, 862009, 8911670, 869691, 8911163, 8911164, 8915, 891703, 8811842, 845636, 8610629, 891716, 859464, 859465, 868682, 87880, 862028, 8610637, 8812877, 859471, 859983, 86355, 862548, 883539, 874839, 8611161, 858970, 8711002, 8711003, 857438, 859487, 869218, 858981, 85638502, 875878, 8510824, 865128, 858986, 868202, 869224, 858477, 874858, 865137, 866674, 873843, 88466802, 889719, 87930, 8911230, 8610175, 868223, 8712064, 8910720, 8910721, 874373, 869254, 86408, 86409, 8711561, 84348301, 873357, 887181, 846226, 884626, 8912280, 853401, 866714, 866203, 867739, 854941, 85759902, 861597, 861598, 871001501, 871001502, 873885, 875938, 877989, 879523, 8910251, 864685, 877486, 861103, 857010, 859575, 8911800, 877500, 877501, 8810436, 888264, 85382601, 8910748, 8810955, 8611792, 861648, 84610002, 884689, 884180, 886226, 864726, 864729, 8510426, 868826, 88330202, 8911834, 8670, 881046502, 8810987, 852973, 892399, 854002, 86517, 886776, 8912284, 862717, 85922302}\n",
      "\n",
      " features: {'area(worst)', 'fractal dimension(worst)', 'fractal dimension(mean)', 'smoothness(stderr)', 'compactness(mean)', 'fractal dimension(stderr)', 'perimeter(worst)', 'symmetry(worst)', 'texture(mean)', 'radius(stderr)', 'texture(stderr)', 'concavity(mean)', 'symmetry(stderr)', 'compactness(worst)', 'concave points(mean)', 'concave points(worst)', 'radius(mean)', 'concavity(worst)', 'area(mean)', 'smoothness(mean)', 'concave points(stderr)', 'texture(worst)', 'perimeter(stderr)', 'concavity(stderr)', 'symmetry(mean)', 'smoothness(worst)', 'compactness(stderr)', 'radius(worst)', 'area(stderr)', 'perimeter(mean)'}\n"
     ]
    }
   ],
   "source": [
    "from vec import Vec\n",
    "from cancer_data import read_training_data\n",
    "\n",
    "A, b = read_training_data('train.data')\n",
    "\n",
    "print('the number of patient:', len(b.D))\n",
    "print('\\n patient ids:', A.D[0])\n",
    "print('\\n features:', A.D[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.2 지도학습(Supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기(classifier)를 선택하는 프로그램을 작성하는 것이다. 예측은 함수 C(y)이다. 프로그램은 라벨을 붙인 예제 $(a_i,b_i),...,(a_m,b_m)$으로 구성된 트레이닝 테디엍가 주어진다. 각각의 라벨을 붙인 예제는 피처 벡터 $a_i$와 대응하는 라벨 $b_i$로 구성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.3 가설 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기는 가능한 분류기들의 집합(가설 클래스)에서 선택된다. 가설 클래스는 피처 벡터들의 공간 $\\mathbb R^D$에서 $\\mathbb R$로의 선형함수 $h(\\cdot)$로 구성된다. 분류기는 이러한 함수에 대해 아래와 같이 정의된다.\n",
    "\n",
    "$\\quad C(y)=\\begin{cases}  +1, if h(y) \\ge 0 \\\\ -1, if h(y) < 0  \\end{cases}$  \n",
    "\n",
    "\n",
    "각각의 선형함수 $h:\\mathbb R^D \\rightarrow \\mathbb R$에 대해, 다음을 만족하는 D-벡터 w가 있다.\n",
    "\n",
    "$\\quad h(y)=w\\cdot y$\n",
    "\n",
    "따라서, 이러한 선형함수를 선택하는 것은 D-벡터 w를 선택하는 것이 된다. w를 선택하는 것은 가설 h를 선택하는 것과 같으므로, w는 **가설벡터**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.2\n",
    "다음 스펙을 가지는 프로시저, signum(u)을 작성해 보자.\n",
    "- input: Vec u\n",
    "- output: u와 동일한 정의역을 가지며 다음을 만족하는 Vec v  \n",
    "$\\quad\\quad v[d]=\\begin{cases}  +1, if u[d] \\ge 0 \\\\ -1, if u[d] < 0  \\end{cases}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({'A', 'B'},{'A': 1, 'B': -1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def signum(u): return Vec(u.D, {k:1 if u[k] >=0 else -1 for k in u.D})\n",
    "\n",
    "u = Vec({'A','B'}, {'A':3, 'B':-2})\n",
    "signum(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.3\n",
    "다음 스펙을 가지는 프로시저, fraction_wrong(A, b, w)을 작성해 보자.\n",
    "- input \n",
    " - 행들이 피처 벡터들인 RxC 행렬 A\n",
    " - 엔트리들이 +1, -1인 R-벡터 b\n",
    " - C-벡터 w\n",
    "- output\n",
    " - A의 행 라벨 r중 (A의 행 r)$\\cdot w$의 부호가 b[r]과 다른 것의 비율을 나타내는 분수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction wrong by random initialization: 0.48\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def fraction_wrong(A, b, w):\n",
    "    pred = signum(A*w)\n",
    "    wrong = [1 if pred[i]!=b[i] else 0 for i in b.D]\n",
    "    return sum(wrong)/len(b.D)\n",
    "\n",
    "w = Vec(A.D[1], {k:1 if random.random() >= 0.5 else -1 for k in A.D[1]})\n",
    "print('fraction wrong by random initialization:', fraction_wrong(A, b, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.4 트레이닝 데이터에 대한 에러를 최소화하는 분류기 선택하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 lab에서는 아주 기본적인 에러 측정 방법을 사용한다. 각각의 라벨을 붙인 예제 $(a_i,b_i)$에 대해, 그 예제의 대한 h의 에러는 $(h(a_i)-b_i)^2$이다. 트레이닝 데이터에 대한 전체 에러는 예제 각각에 대한 에러의 합이다.\n",
    "\n",
    "$\\quad (h(a_1)-b_1)^2+...+(h(a_m)-b_m)^2$\n",
    "\n",
    "함수 $h(\\cdot)$를 선택하는 것은 D-벡터 w를 선택하는 것과 같고 $h(y)=y\\cdot w$라고 정의했다. 대응하는 에러는 다음과 같다.\n",
    "\n",
    "$\\quad (a_1\\cdot w-b_1)^2+...+(a_m\\cdot w-b_m)^2$\n",
    "\n",
    "알고리즘의 목적은 함수 $L:\\mathbb R^D\\rightarrow \\mathbb R$을 다음과 같이 정의해 보자.\n",
    "\n",
    "$\\quad L(x)=(a_1\\cdot x-b_1)^2+...+(a_m\\cdot x-b_m)^2$\n",
    "\n",
    "이 함수는 손실(loss)함수이다. 이 학습 알고리즘의 목적은 L(w)이 가능한 작게 되도록 하는 가설 벡터 w를 선택하는 것이다.\n",
    "\n",
    "이러한 특정 손실함수를 선택하는 한 가지 이유는 이 함수가 이 책에서 다루는 선형대수에 연관될 수 있기 때문이다. A는 행들이 트레이닝 예제들 $a_1,..,a_m$인 행렬이라 하자. b는 m-벡터라고 하고 i번째 엔트리 i는 $b_i$라고 하자. w는 D-벡터라고 하자. 이것은 $\\left\\|Aw-b\\right\\|^2$을 최소화하는 벡터 w를 선택하는 여기서의 목적에 맞는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.4\n",
    "프로시저, loss(A, b, w)를 작성해 보자.\n",
    "- input \n",
    " - 행들이 피처 벡터들인 RxC 행렬 A\n",
    " - 엔트리들이 +1, -1인 R-벡터 b\n",
    " - C-벡터 w\n",
    "- output\n",
    " - w에 대한 손실함수의 값 L(w)을 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss by random initialization: 873343724.8296386\n"
     ]
    }
   ],
   "source": [
    "def loss(A, b, w):\n",
    "    l = (A*w-b)\n",
    "    return l*l\n",
    "\n",
    "print('loss by random initialization:', loss(A, b, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.5 힐 클라이밍(Hill-climbing)에 의한 비선형 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수의 최소화를 찾는데 일반적(generic)이고 흔하게 사용되는 휴리스특(heuristic)인 힐 클라이밍을 사용한다. 이것을 일반적이라고 하는 이유는 아주 넓은 범위의 함수에 대해 사용될 수 있기 때문이다. 하지만, 휴리스틱이라고 하는 이유는 이 방식이 진정한 최소값을 찾는다는 것을 보장할 수 없기 때문이다.\n",
    "\n",
    "**Algorithm**  \n",
    "*initialize w to something*  \n",
    "*repeat as many as you have patience for:*  \n",
    "*$\\quad$ w := w + change*  \n",
    "*return w*  \n",
    "\n",
    "만약 함수를 극대(극소)화하는 것을 찾으려고 한다면, 알고리즘은 솔루션 w를 점점 지형의 꼭대기(골짜기)로 이동한다. 그래서 이름이 힐 클라이밍이다. 힐 클라이밍은 전역(global)이 아닌 국부(local)최소 지점에 빠질 수 있다. 이것이 힐 클라이밍의 바람직하지 못한 측면이지만 피할수도 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.6 그래디언트(Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.4.5\n",
    "최소화할 함수가 선형함수, 예를 들어 $f(w)=c\\cdot w$라고 해 보자. $c\\cdot u < 0$를 만족하는 어떤 벡터 u를 더하여 w를 변경한다고 해보자. 그러면 $f(w+u)<f(w)$이고 그래서 w+u를 w에 할당함으로써 진행할 것이다. u 방향으로 움직이는 것은 함수의 값을 감소시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "최소화될 함수는 선형함수가 아니라면, 올바른 방향은 현재 어디에 있는지에 따라 다르다. 각각의 특정 지점 w에 대해, 그 지점에서 내려가는 경사도가 가장 방향으로 움직여야 한다.\n",
    "\n",
    "함수 $f:\\mathbb R^n\\rightarrow \\mathbb R$에 대해, f의 **그래디언트(gradient)는 $\\nabla f$라고 쓰며 $\\mathbb R^n$에서 $\\mathbb R^n$으로의 함수이다.** 이 함수는 단일 숫자가 아니라 벡터를 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition 9.4.6\n",
    "$f([x_1,...,x_n])$의 그래디언트는 아래와 같이 정의된다.  \n",
    "$\\quad [\\frac{\\partial f}{\\partial x_1},...,\\frac{\\partial f}{\\partial x_n}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.4.7\n",
    "f는 선형함수: $f(x)=c\\cdot x$라 하자. $x_i$에 대한 편도함수(partial derivative)는 $c_i$이다. 그러므로, $\\nabla f([x_1,...,x_n])=[c_1,...,c_n]$이다. 이 함수는 그 인자를 무시하며 그래디언트는 모든 곳에서 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.4.8\n",
    "선형함수가 아닌 함수가 있다고 해 보자. 벡터 a와 스칼라 b에 대해, $f(x)=(a\\cdot x- b)^2$라고 정의하자. $j=1,...,n$에 대해, 다음이 성립한다.\n",
    "\n",
    "$\\quad \\frac{\\partial f}{\\partial x_j} = 2(a\\cdot x - b)\\frac{\\partial}{\\partial x_j}(a\\cdot x-b) = 2(a\\cdot x - b)a_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "다음의 손실함수를 살펴보자.\n",
    "\n",
    "이 손실함수를 선택한 한 가지 이유는 그 편도함수가 존재하고 계산하기 쉽기 때문이다.\n",
    "\n",
    "$\\quad L(x)=\\sum_{i=1}^{m}{(\\mathbf a_i\\cdot \\mathbf x-b_i)^2}$\n",
    "\n",
    "편도함수는 아래와 같다.\n",
    "\n",
    "$\\quad \\frac{\\partial L}{\\partial x_j}=\\sum_{i=1}^{m}{(\\mathbf a_i\\cdot \\mathbf x-b_i)a_{ij}}$  \n",
    "여기서 $a_{ij}$는 $a_i$의 엔트리 j이다.\n",
    "\n",
    "따라서, 벡터 w에 대한 그래디언트 함수값은 벡터이며 이 벡터의 엔트리는 다음과 같다.\n",
    "\n",
    "$\\quad \\sum_{i=1}^{m}{2(\\mathbf a_i\\cdot \\mathbf w-b_i)a_{ij}}$  \n",
    "\n",
    "즉 벡터는 다음과 같다.\n",
    "\n",
    "$\\quad \\nabla L(w)=[\\sum_{i=1}^{m}{(\\mathbf a_i\\cdot \\mathbf w-b_i)a_{i1}}, ..., \\sum_{i=1}^{m}{(\\mathbf a_i\\cdot \\mathbf w-b_i)a_{in}}]$  \n",
    "\n",
    "위 식은 벡터 덧셈을 사용하여 다음과 같이 다시 쓸 수 있다.\n",
    "\n",
    "$\\quad \\sum_{i=1}^{m}{2(\\mathbf a_i\\cdot \\mathbf w-b_i)\\mathbf a_i}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.9\n",
    "프로시저, find_grad(A, b, w)을 작성해 보자.\n",
    "- input \n",
    " - 행들이 피처 벡터들인 RxC 행렬 A\n",
    " - 엔트리들이 +1, -1인 R-벡터 b\n",
    " - C-벡터 w\n",
    "- output\n",
    " - w 지점에서의 L의 그래디언트값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({'area(worst)', 'fractal dimension(worst)', 'fractal dimension(mean)', 'smoothness(stderr)', 'compactness(mean)', 'fractal dimension(stderr)', 'perimeter(worst)', 'symmetry(worst)', 'texture(mean)', 'radius(stderr)', 'texture(stderr)', 'concavity(mean)', 'symmetry(stderr)', 'compactness(worst)', 'concave points(mean)', 'concave points(worst)', 'radius(mean)', 'concavity(worst)', 'area(mean)', 'smoothness(mean)', 'concave points(stderr)', 'texture(worst)', 'perimeter(stderr)', 'concavity(stderr)', 'symmetry(mean)', 'smoothness(worst)', 'compactness(stderr)', 'radius(worst)', 'area(stderr)', 'perimeter(mean)'},{'area(worst)': -1123572182.2259822, 'fractal dimension(worst)': -75113.95226160964, 'fractal dimension(mean)': -53757.070114933376, 'smoothness(stderr)': -5936.891419180288, 'compactness(mean)': -111371.40594982226, 'fractal dimension(stderr)': -3425.940704807801, 'perimeter(worst)': -114535831.16204365, 'symmetry(worst)': -267071.2050498865, 'texture(mean)': -17770627.750757188, 'radius(stderr)': -493176.8503868593, 'texture(stderr)': -1042470.4044961105, 'concavity(mean)': -115115.6956798416, 'symmetry(stderr)': -18023.85019054586, 'compactness(worst)': -276396.5455275784, 'concave points(mean)': -64033.81279499237, 'concave points(worst)': -133940.5747837635, 'radius(mean)': -14413955.989898846, 'concavity(worst)': -318566.08842280915, 'area(mean)': -781949687.1307238, 'smoothness(mean)': -86341.09338073332, 'concave points(stderr)': -11841.089578039366, 'texture(worst)': -23860168.262600295, 'perimeter(stderr)': -3498782.851455507, 'concavity(stderr)': -32681.771399607478, 'symmetry(mean)': -163386.9535896374, 'smoothness(worst)': -119730.0534254049, 'compactness(stderr)': -25815.248374009538, 'radius(worst)': -17236231.22959346, 'area(stderr)': -58167548.24942555, 'perimeter(mean)': -94738286.67874637})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_grad(A, b, w):\n",
    "    return 2*(A*w-b)*A\n",
    "\n",
    "find_grad(A, b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4.7 그래디언트 디센트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래디언트 디센트의 개념은 벡터 w를 반복적으로 업데이트하는 것이다. 각 이터레이션에서, 알고리즘은 w에서의 그래디언트 값의 음수를 작은 스칼라로 곱한 것을 w에 더한다. 이때, 이 스칼라는 스텝 크기(step size)라 하고 $\\sigma$로 나타낸다.\n",
    "\n",
    "**Algorithm**  \n",
    "*Set $\\sigma$ to be a small number*  \n",
    "*Initialize w to be some D-vector*  \n",
    "*repeat some number of times:*  \n",
    "*$\\quad$ w := w + $\\sigma(\\nabla L(w))$*  \n",
    "*return w*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.10\n",
    "프로시저, gradient_descent_step(A, b, w, sigma)를 작성해 보자.\n",
    "- input \n",
    " - 행들이 피처 벡터들인 RxC 행렬 A\n",
    " - 엔트리들이 +1, -1인 R-벡터 b\n",
    " - C-벡터 w\n",
    " - 스텝 크기 sigma\n",
    "- output\n",
    " - 그래디언트를 스텝 크기와 곱하여 그 결과를 w에 뺀 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vec({'area(worst)', 'fractal dimension(worst)', 'fractal dimension(mean)', 'smoothness(stderr)', 'compactness(mean)', 'fractal dimension(stderr)', 'perimeter(worst)', 'symmetry(worst)', 'texture(mean)', 'radius(stderr)', 'texture(stderr)', 'concavity(mean)', 'symmetry(stderr)', 'compactness(worst)', 'concave points(mean)', 'concave points(worst)', 'radius(mean)', 'concavity(worst)', 'area(mean)', 'smoothness(mean)', 'concave points(stderr)', 'texture(worst)', 'perimeter(stderr)', 'concavity(stderr)', 'symmetry(mean)', 'smoothness(worst)', 'compactness(stderr)', 'radius(worst)', 'area(stderr)', 'perimeter(mean)'},{'area(worst)': 1123571.1822259822, 'fractal dimension(worst)': 76.11395226160964, 'fractal dimension(mean)': 52.75707011493338, 'smoothness(stderr)': 6.936891419180288, 'compactness(mean)': 112.37140594982226, 'fractal dimension(stderr)': 4.425940704807801, 'perimeter(worst)': 114536.83116204364, 'symmetry(worst)': 266.0712050498865, 'texture(mean)': 17771.627750757187, 'radius(stderr)': 494.1768503868593, 'texture(stderr)': 1043.4704044961106, 'concavity(mean)': 116.11569567984161, 'symmetry(stderr)': 17.02385019054586, 'compactness(worst)': 277.3965455275784, 'concave points(mean)': 63.03381279499237, 'concave points(worst)': 134.9405747837635, 'radius(mean)': 14412.955989898846, 'concavity(worst)': 317.5660884228092, 'area(mean)': 781948.6871307238, 'smoothness(mean)': 87.34109338073333, 'concave points(stderr)': 12.841089578039366, 'texture(worst)': 23861.168262600295, 'perimeter(stderr)': 3497.7828514555067, 'concavity(stderr)': 33.68177139960748, 'symmetry(mean)': 162.3869535896374, 'smoothness(worst)': 120.7300534254049, 'compactness(stderr)': 24.81524837400954, 'radius(worst)': 17235.23122959346, 'area(stderr)': 58166.548249425556, 'perimeter(mean)': 94739.28667874637})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent_step(A, b, w, sigma):\n",
    "    return w - sigma * find_grad(A, b, w)\n",
    "\n",
    "gradient_descent_step(A, b, w, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.11\n",
    "프로시저, gradient_descent(A, b, w, sigma, T)을 작성해 보자.\n",
    "- input \n",
    " - 행들이 피처 벡터들인 RxC 행렬 A\n",
    " - 엔트리들이 +1, -1인 R-벡터 b\n",
    " - C-벡터 w\n",
    " - 스텝 크기 sigma\n",
    " - 이터레이션 수 T\n",
    "- output\n",
    " - 이터레이션 이후 변화된 최종값 w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(A, b, w, sigma, T):\n",
    "    for i in range(T):\n",
    "        w = gradient_descent_step(A, b, w, sigma)\n",
    "        if i % 1000 == 0:\n",
    "            print('loss:', loss(A, b, w))\n",
    "            print('fraction_wrong:', fraction_wrong(A, b, w))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.12\n",
    "작성한 그래디언트 디센트 코드를 트레이닝 데이터에 대해 시험해 보자. 옳지 않는 비율은 심지어 손실함수 값이 줄어들 때에도 증가할 수 있음에 주의 하자. 결국에는 손실 함수의 값이 계속해서 감소함에 따라 옳지 않는 비율도 감소해야 한다.  \n",
    "스텝 크기가 너무 클 경우 손실값이 줄어즐지 않을 수 있으며, 너무 작으면 이터레이션 횟수가 클 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is intialized as one-vector\n",
      "loss: 2908876.2425468992\n",
      "fraction_wrong: 0.6166666666666667\n",
      "loss: 400006.63080027106\n",
      "fraction_wrong: 0.67\n",
      "loss: 191862.7479961799\n",
      "fraction_wrong: 0.6233333333333333\n",
      "loss: 101785.55915446648\n",
      "fraction_wrong: 0.59\n",
      "loss: 57146.61973702796\n",
      "fraction_wrong: 0.5733333333333334\n",
      "loss: 33636.59436504275\n",
      "fraction_wrong: 0.5766666666666667\n",
      "loss: 20927.669257789446\n",
      "fraction_wrong: 0.5566666666666666\n",
      "loss: 13958.006749020538\n",
      "fraction_wrong: 0.5533333333333333\n",
      "loss: 10083.292669421002\n",
      "fraction_wrong: 0.5233333333333333\n",
      "loss: 7887.740676022556\n",
      "fraction_wrong: 0.5\n",
      "\n",
      "\n",
      "w is intialized as zero-vector\n",
      "loss: 267.8325958362454\n",
      "fraction_wrong: 0.5133333333333333\n",
      "loss: 190.19326736917856\n",
      "fraction_wrong: 0.11333333333333333\n",
      "loss: 181.237323991281\n",
      "fraction_wrong: 0.13333333333333333\n",
      "loss: 178.44530845715053\n",
      "fraction_wrong: 0.13666666666666666\n",
      "loss: 177.20373503619243\n",
      "fraction_wrong: 0.13\n",
      "loss: 176.4645362206089\n",
      "fraction_wrong: 0.14\n",
      "loss: 175.93501840669356\n",
      "fraction_wrong: 0.14666666666666667\n",
      "loss: 175.50867489192274\n",
      "fraction_wrong: 0.15666666666666668\n",
      "loss: 175.1375999764591\n",
      "fraction_wrong: 0.16\n",
      "loss: 174.7976415634294\n",
      "fraction_wrong: 0.16\n"
     ]
    }
   ],
   "source": [
    "T = 10000\n",
    "sigma = 9*10e-11\n",
    "\n",
    "print('w is intialized as one-vector')\n",
    "w = Vec(A.D[1], {k:1 for k in A.D[1]})\n",
    "w = gradient_descent(A, b, w, sigma, T)\n",
    "\n",
    "print('\\n\\nw is intialized as zero-vector')\n",
    "w = Vec(A.D[1], {k:0 for k in A.D[1]})\n",
    "w = gradient_descent(A, b, w, sigma, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9.4.13\n",
    "작성한 그래디언트 디센트 코드를 사용하여 가설벡터 w를 찾은 후, validate.data에 대해 얼마나 잘 동작하는지 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction wrong: 0.05384615384615385\n"
     ]
    }
   ],
   "source": [
    "A_val, b_val = read_training_data('validate.data')\n",
    "\n",
    "print('fraction wrong:', fraction_wrong(A_val, b_val, w))\n",
    "# train data보다 일반적이지 않지만 성공률이 높다 .\n",
    "# train data은 모든 피처 벡터 공간을 나타내지 않기 때문에 validata data와 분포가 다를 수 있고 따라서 해 공간도 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.5 Review question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q) $\\mathbb R$상의 벡터들에 대한 내적은 무엇인가?\n",
    "- $\\mathbb R$상의 두 벡터 u, v의 내적 = $<u, v> = u\\cdot v$, 즉 dot product 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q) norm은 도트곱에 대해 어떻게 정의되는가?\n",
    "- $\\left\\|v\\right\\|^2 = \\sqrt{<v, v>} = v\\cdot v$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q) 두 벡터가 직교한다는 것은 무엇을 의미하는가?\n",
    "- $\\mathbb R$상이 두 벡터 u, v가 직교한다면 $<u, v> = 0$, 즉 내적이 0이라는 의미이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q) 벡터들에 대한 피타고라스 정리는 무엇인가?\n",
    "- 만약 실수 벡터 u와 v가 직교하면 다음이 성립한다.  \n",
    "$\\quad \\left\\|u+v\\right\\|^2=\\left\\|u\\right\\|^2+\\left\\|v\\right\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q) 벡터의 평해-수직 분해는 무엇인가?\n",
    "- $b=b^{\\parallel v} + b^{\\bot v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q) 어떤 벡터 v에 직교하는 벡터 b의 투영은 어떻게 찾는가?\n",
    "- $< b^{\\bot v}, v> = 0$  \n",
    "$\\rightarrow \\quad <b - b^{\\parallel v}, v> = 0$  \n",
    "$\\rightarrow \\quad <b - \\sigma v, v> = 0$  \n",
    "$\\rightarrow \\quad <b, v> - \\sigma < v, v> = 0$  \n",
    "$\\rightarrow \\quad \\sigma <v, v> = <b, v>$  \n",
    "$\\rightarrow \\quad \\sigma  = \\frac{<b, v>}{<v,v>}$  \n",
    "$\\rightarrow \\quad b^{\\bot v} = b - \\frac{<b, v>}{<v,v>}v$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q) 선형대수는 비선형함수를 최적화나는 데 어떻게 도움이 되는가?\n",
    "- 손실함수를 선형대수를 이용한 norm 또는 내적으로 정의할 수 있고, 비선형함수에 대해서 gradient descent 방법을 이용하여 norm을 줄여가는 방향으로 가면서 최적화할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.6 Problmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.6.1 \n",
    "다음의 각 문제에 대해, 주어진 벡터 v의 norm을 계산하여라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) norm: 3.0\n",
      "(b) norm: 4.0\n",
      "(c) norm: 3.0\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# (a)\n",
    "v = list2vec([2,2,1])\n",
    "print('(a) norm:', sqrt(v*v))\n",
    "\n",
    "# (b)\n",
    "v = list2vec([sqrt(2),sqrt(3),sqrt(5), sqrt(6)])\n",
    "print('(b) norm:', sqrt(v*v))\n",
    "\n",
    "# (c)\n",
    "v = list2vec([1,1,1,1,1,1,1,1,1])\n",
    "print('(c) norm:', sqrt(v*v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.6.2 \n",
    "다음의 a, b의 각각에 대해, Span{a} 내에 있으며 b에 가장 가까운 벡터를 찾아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. projection b along a: \n",
      "   0   1\n",
      "--------\n",
      " 1.6 3.2\n",
      "\n",
      "2. projection b along a: \n",
      " 0 1 2\n",
      "------\n",
      " 0 1 0\n",
      "\n",
      "3. projection b along a: \n",
      " 0 1 2  3\n",
      "---------\n",
      " 3 2 1 -4\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "a = list2vec([1,2])\n",
    "b = list2vec([2,3])\n",
    "print('1. projection b along a:', (b*a)/(a*a)*a)\n",
    "\n",
    "# 2.\n",
    "a = list2vec([0, 1, 0])\n",
    "b = list2vec([1.414, 1, 1.732])\n",
    "print('\\n2. projection b along a:', (b*a)/(a*a)*a)\n",
    "\n",
    "# 3.\n",
    "a = list2vec([-3, -2, -1, 4])\n",
    "b = list2vec([7,2,5,0])\n",
    "print('\\n3. projection b along a:', (b*a)/(a*a)*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9.6.2 \n",
    "다음 a, b 각각에 대해, $b^{\\parallel a}, b^{\\bot a}$을 찾아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. b_parallel_a: \n",
      " 0 1\n",
      "----\n",
      " 2 0\n",
      "b_orthogonal_b: \n",
      " 0 1\n",
      "----\n",
      " 0 1\n",
      "\n",
      "2. b_parallel_a: \n",
      "      0      1     2\n",
      "--------------------\n",
      " -0.167 -0.333 0.167\n",
      "b_orthogonal_b: \n",
      "    0    1    2\n",
      "---------------\n",
      " 1.17 1.33 3.83\n",
      "\n",
      "3. b_parallel_a: \n",
      " 0 1 2\n",
      "------\n",
      " 1 1 4\n",
      "b_orthogonal_b: \n",
      " 0 1 2\n",
      "------\n",
      " 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "a = list2vec([3,0])\n",
    "b = list2vec([2,1])\n",
    "b_parallel_a = (b*a)/(a*a)*a\n",
    "b_orthogonal_a = b - b_parallel_a\n",
    "print('1. b_parallel_a:', b_parallel_a)\n",
    "print('b_orthogonal_b:', b_orthogonal_a)\n",
    "\n",
    "# 2.\n",
    "a = list2vec([1,2,-1])\n",
    "b = list2vec([1,1, 4])\n",
    "b_parallel_a = (b*a)/(a*a)*a\n",
    "b_orthogonal_a = b - b_parallel_a\n",
    "print('\\n2. b_parallel_a:', b_parallel_a)\n",
    "print('b_orthogonal_b:', b_orthogonal_a)\n",
    "\n",
    "# 3.\n",
    "a = list2vec([3,3,12])\n",
    "b = list2vec([1,1,4])\n",
    "b_parallel_a = (b*a)/(a*a)*a\n",
    "b_orthogonal_a = b - b_parallel_a\n",
    "print('\\n3. b_parallel_a:', b_parallel_a)\n",
    "print('b_orthogonal_b:', b_orthogonal_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
