{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Artistic Style Transfer: A Comprehensive Look\n",
    "-https://medium.com/artists-and-machine-intelligence/neural-artistic-style-transfer-a-comprehensive-look-f54d8649c199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram matrix layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        a,b,c,d = input.size()\n",
    "        features = input.view(a*b, c*d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        \n",
    "        return G.div(a*b*c*d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class StyleCNN(object):\n",
    "    def __init__(self, style, content, pastiche):\n",
    "        super(StyleCNN, self).__init__()\n",
    "        \n",
    "        self.style = style\n",
    "        self.content = content\n",
    "        self.pastiche = nn.Parameter(pastiche.data)\n",
    "        \n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 1\n",
    "        self.style_weight = 1000\n",
    "        \n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "        \n",
    "        self.gram = GramMatrix()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.LBFGS([self.pastiche])    \n",
    "        \n",
    "    def train(self):\n",
    "        def closure():\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            pastiche = self.pastiche.clone()\n",
    "            pastiche.data.clamp_(0, 1)\n",
    "            content = self.content.clone()\n",
    "            style = self.style.clone()\n",
    "            \n",
    "            content_loss = 0\n",
    "            style_loss = 0\n",
    "            \n",
    "            i = 1\n",
    "            not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "            for layer in list(self.loss_network.features):\n",
    "                layer = not_inplace(layer)\n",
    "                pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "                \n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    name = 'conv_' + str(i)\n",
    "                    \n",
    "                    if name in self.content_layers:\n",
    "                        content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "                        \n",
    "                    if name in self.style_layers:\n",
    "                        pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                        style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "                        \n",
    "                if isinstance(layer, nn.ReLU):\n",
    "                    i += 1\n",
    "            \n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "                \n",
    "            return total_loss\n",
    "        \n",
    "        self.optimizer.step(closure)\n",
    "        return self.pastiche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "loader = transforms.Compose([transforms.Resize((imsize, imsize)),\n",
    "                             transforms.ToTensor()])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def save_image(input, path):\n",
    "    image = input.data.clone().cpu()\n",
    "    image = image.view(3, imsize, imsize)\n",
    "    image = unloader(image)\n",
    "    scipy.misc.imsave(path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [32:46<00:00, 65.83s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "    # Content and style\n",
    "    style = image_loader('starry_night.jpg').type(dtype)\n",
    "    content = image_loader('dancing.jpg').type(dtype)\n",
    "\n",
    "    pastiche = image_loader('dancing.jpg').type(dtype)\n",
    "    pastiche.data = torch.randn(pastiche.data.size()).type(dtype)\n",
    "\n",
    "    num_epochs = 31\n",
    "\n",
    "    style_cnn = StyleCNN(style, content, pastiche)\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        pastiche = style_cnn.train()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            path = 'dancing_s%d.png' % (i)\n",
    "            pastiche.data.clamp_(0, 1)\n",
    "            save_image(pastiche, path)\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Transformation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleCNN(object):\n",
    "    def __init__(self, style):\n",
    "        super(StyleCNN, self).__init__()\n",
    "        \n",
    "        self.style = style\n",
    "        \n",
    "        self.content_layers = ['conv4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 1\n",
    "        self.style_weight = 1000\n",
    "        \n",
    "        self.transform_network = nn.Sequential(nn.ReflectionPad2d(40),\n",
    "                                               nn.Conv2d(3, 32, 9, stride=1, padding=4),\n",
    "                                               nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                                               nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                               nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "                                               nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "                                               nn.Conv2d(32, 3, 9, stride=1, padding=4),\n",
    "                                               )\n",
    "        \n",
    "        self.gram = GramMatrix()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.transform_network.parameters(), lr=1e-3)    \n",
    "        \n",
    "    def train(self, content_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        style = self.style.clone()\n",
    "        \n",
    "        for content in content_batch:\n",
    "            pastiche = self.transform_network.forward(content)\n",
    "\n",
    "            content_loss = 0\n",
    "            style_loss = 0\n",
    "\n",
    "            i = 1\n",
    "            not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "            for layer in list(self.transform_network):\n",
    "                layer = not_inplace(layer)\n",
    "                pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    name = 'conv_' + str(i)\n",
    "\n",
    "                    if name in self.content_layers:\n",
    "                        content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "\n",
    "                    if name in self.style_layers:\n",
    "                        pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                        style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "\n",
    "                if isinstance(layer, nn.ReLU):\n",
    "                    i += 1\n",
    "\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return self.pastiche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define save_images for batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(input, path):\n",
    "    N = input.size()[0]\n",
    "    image = input.data.clone().cpu()\n",
    "    for n in range(N):\n",
    "        image = images[n]\n",
    "        image = image.view(3, imsize, imsize)\n",
    "        image = unloader(image)\n",
    "        scipy.misc.imsave(paths[n], image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main for ITN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    style = image_loader('starry_night.jpg').type(torch.FloatTensor)\n",
    "    style_cnn = StyleCNN(style)\n",
    "    num_epochs = 3\n",
    "    N = 4\n",
    "    \n",
    "    # Contents\n",
    "    coco = datasets.ImageFolder(root='./', transform=loader)\n",
    "    content_loader = torch.utils.data.DataLoader(coco, batch_size=N, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, content_batch in enumerate(content_loader):\n",
    "            iteration = epoch * i + i\n",
    "            print(len(content_batch[1]))\n",
    "            content_loss, style_loss, pastiches = style_cnn.train(content_batch)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(\"Iteration: %d\" % (iteration))\n",
    "                print(\"Content loss: %f\" % (content_loss.data[0]))\n",
    "                print(\"Style loss: %f\" % (style_loss.data[0]))\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                path = \"outputs/%d_\" % (iteration)\n",
    "                paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "                save_images(pastiches, paths)\n",
    "\n",
    "                path = \"outputs/content_%d_\" % (iteration)\n",
    "                paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "                save_images(content_batch, paths)\n",
    "                style_cnn.save()\n",
    "                \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
